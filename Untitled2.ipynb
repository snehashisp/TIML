{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from queue import Queue\n",
    "import copy\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBatch():\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.replay_buf = {}\n",
    "        self.replay_index = 0\n",
    "        self.current_buf_size = 0\n",
    "        \n",
    "    def store_replay(self, replay):\n",
    "        if self.replay_index == self.buffer_size:\n",
    "            self.replay_index = 0\n",
    "        self.replay_buf[self.replay_index] = copy.deepcopy(replay)\n",
    "        self.replay_index += 1\n",
    "        self.current_buf_size = max(self.replay_index, self.current_buf_size)\n",
    "        \n",
    "    def getRandomBatch(self, batch_size):\n",
    "        batch = np.random.permutation(self.current_buf_size)[:batch_size]\n",
    "        replay_batch = []\n",
    "        for b in batch:\n",
    "            replay_batch += [self.replay_buf[b]]\n",
    "        return replay_batch\n",
    "    \n",
    "    def reset(self):\n",
    "        del(self.replay_buf)\n",
    "        self.replay_buf = {}\n",
    "        self.replay_index = 0\n",
    "        self.current_buf_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImg(state_images):\n",
    "    images = []\n",
    "    for img in state_images:\n",
    "        images.append(cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), dsize = (84, 110))[101 - 84:-9])\n",
    "    return np.dstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, state_shape, actions, lrate = 0.00025, \n",
    "                 momentum = 0.95, discount = 0.95, save_file = None, load_file = None):\n",
    "        tf.reset_default_graph()\n",
    "        self._sess = tf.Session(config = config)\n",
    "        self.state_shape = state_shape\n",
    "        self.actions = actions\n",
    "        self.save_file = save_file\n",
    "        self.load_file = load_file\n",
    "        self.discount = discount\n",
    "        self._input = tf.placeholder(tf.float32, [None, state_shape[0], state_shape[1], state_shape[2]])\n",
    "        self._action_value = tf.placeholder(tf.float32, [None, actions])\n",
    "        self._initLayers()\n",
    "        self.pred = self._predict()\n",
    "        self.loss = keras.losses.MSE(self._action_value, self.pred)\n",
    "        self.train_step = tf.train.AdamOptimizer(lrate).minimize(self.loss)\n",
    "        #print(tf.global_variables())\n",
    "        if load_file is None:\n",
    "            self._sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "    def _initLayers(self):\n",
    "        self.conv1 = keras.layers.Conv2D(16, 8, strides = (4, 4), padding = 'valid', \n",
    "                                         activation = 'relu', \n",
    "                                         input_shape = self.state_shape,\n",
    "                                         kernel_initializer = 'VarianceScaling' \n",
    "                                        )\n",
    "        self.conv2 = keras.layers.Conv2D(32, 4, strides = (2, 2), \n",
    "                                         padding = 'valid', \n",
    "                                         activation = 'relu',\n",
    "                                         kernel_initializer = 'VarianceScaling'\n",
    "                                        )\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.dense1 = keras.layers.Dense(256, activation = 'relu',\n",
    "                                         kernel_initializer = 'VarianceScaling',\n",
    "                                         bias_initializer = 'VarianceScaling'\n",
    "                                        )\n",
    "        self.dense2 = keras.layers.Dense(self.actions,\n",
    "                                         kernel_initializer = 'VarianceScaling',\n",
    "                                         bias_initializer = 'VarianceScaling'\n",
    "                                        )\n",
    "        \n",
    "    def _predict(self):\n",
    "        x = self.flatten(self.conv2(self.conv1(self._input)))\n",
    "        x = self.dense2(self.dense1(x))\n",
    "        return x\n",
    "        \n",
    "    def predict(self, state_data):\n",
    "        state_data_float = state_data/255\n",
    "        assert state_data[0].shape == self.state_shape\n",
    "        pred = self._sess.run([self.pred], feed_dict = {\n",
    "            self._input: state_data_float\n",
    "        })\n",
    "        return pred\n",
    "    \n",
    "    def train(self, replay_batch):\n",
    "        next_state_batch = np.array([i[-2] for i in replay_batch], dtype = np.float32)\n",
    "        current_state_batch = np.array([i[0] for i in replay_batch], dtype = np.float32)\n",
    "        sess = self._sess\n",
    "        next_state_pred = sess.run([self.pred], feed_dict = {\n",
    "            self._input: next_state_batch/255\n",
    "        })\n",
    "        current_state_pred = sess.run([self.pred], feed_dict = {\n",
    "            self._input: current_state_batch/255\n",
    "        })[0]\n",
    "        \n",
    "        #print(current_state_pred.shape)\n",
    "        best_actions = [np.max(actions) for actions in next_state_pred[0]]\n",
    "        #print(len(best_actions))\n",
    "        for i, rb in enumerate(replay_batch):\n",
    "            if not rb[-1]:\n",
    "                current_state_pred[i,rb[1]] = rb[2] + self.discount * best_actions[i]\n",
    "            else:\n",
    "                current_state_pred[i,rb[1]] = rb[2]\n",
    "        _, loss = sess.run([self.train_step, self.loss], feed_dict = {\n",
    "            self._action_value: current_state_pred,\n",
    "            self._input: current_state_batch/255\n",
    "        })\n",
    "        #print(np.sum(loss))\n",
    "        return np.sum(loss)/len(loss)\n",
    "    \n",
    "    def save(self):\n",
    "        assert self.save_file != None\n",
    "        saver = tf.train.Saver(var_list = tf.global_variables())\n",
    "        saver.save(self._sess, self.save_file)\n",
    "        \n",
    "    def load(self):\n",
    "        assert self.load_file != None\n",
    "        loader = tf.train.Saver()\n",
    "        loader.restore(self._sess, self.load_file)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def step(env, action, framecount = 4):\n",
    "    reward = 0\n",
    "    states = []\n",
    "    for i in range(framecount):\n",
    "        state, r, done, p = env.step(action)\n",
    "        states += [state]\n",
    "        reward += r\n",
    "        if done:\n",
    "            if len(states) < 4:\n",
    "                for i in range(4 - len(states)):\n",
    "                    states += [state]\n",
    "            #r = -1\n",
    "            break\n",
    "    processed_images = processImg(states).astype(np.uint)\n",
    "    return [action, reward, processed_images, done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, dqn, fire_interval = 10, render = False):\n",
    "    state = env.reset()\n",
    "    init_state, a, r, p = env.step(1)\n",
    "    state = processImg([init_state, init_state, init_state, init_state])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    fi = fire_interval\n",
    "    while not done:\n",
    "        pred = dqn.predict(np.array([state]))[0]\n",
    "        action = np.argmax(pred)\n",
    "        action, reward, state, done = step(env, action)\n",
    "        total_reward += reward\n",
    "        if render:\n",
    "            time.sleep(0.05)\n",
    "            env.render()\n",
    "        fi -= 1\n",
    "        if fi == 0:\n",
    "            env.step(1)\n",
    "            fi = fire_interval\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dqLearn(env, exp_buf, dqn, eps, epsdecay = 0.9994, episodes = 10000, \n",
    "            train_len = 200, min_buf_size = 2500, buf_clear_thres = 4000,\n",
    "            render_interval = 10, batch_size = 48):\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        \n",
    "        print('Episode ', ep, \"Current Replay Size\", exp_buf.current_buf_size)\n",
    "        env.reset()\n",
    "        init_state, a, r, p = env.step(1)\n",
    "        prev_state = processImg([init_state, init_state, init_state, init_state])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            next_action_choice = np.random.choice([0,1], p = [eps, 1-eps])\n",
    "            if next_action_choice == 0:\n",
    "                next_action = np.random.choice([0,1,2,3])\n",
    "            else:\n",
    "                next_action = np.argmax(dqn.predict(np.array([prev_state]))[0])\n",
    "            replay = step(env, next_action)\n",
    "            done = replay[-1] \n",
    "            replay = [prev_state] + replay\n",
    "            exp_buf.store_replay(replay)\n",
    "            prev_state = replay[-2]\n",
    "            \n",
    "        if exp_buf.current_buf_size >= min_buf_size:\n",
    "            avg_loss = 0\n",
    "            for _ in tqdm(range(train_len)):\n",
    "                train_batch = exp_buf.getRandomBatch(batch_size)\n",
    "                avg_loss += dqn.train(train_batch)\n",
    "            #eps = max(0.1, epsdecay * eps)\n",
    "            print('Average Loss', avg_loss/train_len)\n",
    "            dqn.save()\n",
    "            if render_interval !=0 and ep % render_interval == 0:\n",
    "                print(\"Reward \", play(env, dqn, render = False))\n",
    "#             else:\n",
    "#                 print(\"Reward \", play(env, dqn))\n",
    "                \n",
    "        if exp_buf.current_buf_size >= buf_clear_thres:\n",
    "            exp_buf.reset()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/snehashis/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from my_model4\n"
     ]
    }
   ],
   "source": [
    "benv = gym.make('Breakout-v4')\n",
    "exp_buf = ExperienceReplayBatch(10000)\n",
    "dqn = DQN((84, 84, 4), 4, save_file = 'my_model5', load_file = 'my_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0 Current Replay Size 0\n",
      "Episode  1 Current Replay Size 68\n",
      "Episode  2 Current Replay Size 130\n",
      "Episode  3 Current Replay Size 220\n",
      "Episode  4 Current Replay Size 362\n",
      "Episode  5 Current Replay Size 478\n",
      "Episode  6 Current Replay Size 630\n",
      "Episode  7 Current Replay Size 729\n",
      "Episode  8 Current Replay Size 829\n",
      "Episode  9 Current Replay Size 978\n",
      "Episode  10 Current Replay Size 1139\n",
      "Episode  11 Current Replay Size 1236\n",
      "Episode  12 Current Replay Size 1326\n",
      "Episode  13 Current Replay Size 1422\n",
      "Episode  14 Current Replay Size 1567\n",
      "Episode  15 Current Replay Size 1700\n",
      "Episode  16 Current Replay Size 1863\n",
      "Episode  17 Current Replay Size 1970\n",
      "Episode  18 Current Replay Size 2065\n",
      "Episode  19 Current Replay Size 2202\n",
      "Episode  20 Current Replay Size 2299\n",
      "Episode  21 Current Replay Size 2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.007644126941449943\n",
      "Episode  22 Current Replay Size 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.004013495838347202\n",
      "Episode  23 Current Replay Size 2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0036057578178588286\n",
      "Episode  24 Current Replay Size 2704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002887439000575492\n",
      "Episode  25 Current Replay Size 2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0028237630314348894\n",
      "Episode  26 Current Replay Size 2997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0029915848905996746\n",
      "Episode  27 Current Replay Size 3083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0029230480811869098\n",
      "Episode  28 Current Replay Size 3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0027226501610130073\n",
      "Episode  29 Current Replay Size 3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002720316539440923\n",
      "Episode  30 Current Replay Size 3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0026658537506591535\n",
      "Reward  4.0\n",
      "Episode  31 Current Replay Size 3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0026766888929220536\n",
      "Episode  32 Current Replay Size 3654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0023225492603766414\n",
      "Episode  33 Current Replay Size 3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0023933885421138267\n",
      "Episode  34 Current Replay Size 3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0024818979909953967\n",
      "Episode  35 Current Replay Size 3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 22.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002384245657982925\n",
      "Episode  36 Current Replay Size 0\n",
      "Episode  37 Current Replay Size 111\n",
      "Episode  38 Current Replay Size 242\n",
      "Episode  39 Current Replay Size 321\n",
      "Episode  40 Current Replay Size 436\n",
      "Episode  41 Current Replay Size 574\n",
      "Episode  42 Current Replay Size 652\n",
      "Episode  43 Current Replay Size 786\n",
      "Episode  44 Current Replay Size 940\n",
      "Episode  45 Current Replay Size 1060\n",
      "Episode  46 Current Replay Size 1186\n",
      "Episode  47 Current Replay Size 1318\n",
      "Episode  48 Current Replay Size 1402\n",
      "Episode  49 Current Replay Size 1476\n",
      "Episode  50 Current Replay Size 1561\n",
      "Episode  51 Current Replay Size 1638\n",
      "Episode  52 Current Replay Size 1698\n",
      "Episode  53 Current Replay Size 1817\n",
      "Episode  54 Current Replay Size 1909\n",
      "Episode  55 Current Replay Size 2008\n",
      "Episode  56 Current Replay Size 2181\n",
      "Episode  57 Current Replay Size 2272\n",
      "Episode  58 Current Replay Size 2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.005897907728794964\n",
      "Episode  59 Current Replay Size 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003591580931097273\n",
      "Episode  60 Current Replay Size 2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003218856977764516\n",
      "Reward  9.0\n",
      "Episode  61 Current Replay Size 2850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0030976863473188127\n",
      "Episode  62 Current Replay Size 3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002695317474038651\n",
      "Episode  63 Current Replay Size 3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.00264393959970524\n",
      "Episode  64 Current Replay Size 3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0023633181264934443\n",
      "Episode  65 Current Replay Size 3404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0025546838855370884\n",
      "Episode  66 Current Replay Size 3496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0021928230661433195\n",
      "Episode  67 Current Replay Size 3597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002376447794182848\n",
      "Episode  68 Current Replay Size 3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002225697837226713\n",
      "Episode  69 Current Replay Size 3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0022069939873957394\n",
      "Episode  70 Current Replay Size 3884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0019707330634507027\n",
      "Reward  13.0\n",
      "Episode  71 Current Replay Size 3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.002638959671991568\n",
      "Episode  72 Current Replay Size 0\n",
      "Episode  73 Current Replay Size 69\n",
      "Episode  74 Current Replay Size 199\n",
      "Episode  75 Current Replay Size 350\n",
      "Episode  76 Current Replay Size 421\n",
      "Episode  77 Current Replay Size 489\n",
      "Episode  78 Current Replay Size 581\n",
      "Episode  79 Current Replay Size 689\n",
      "Episode  80 Current Replay Size 809\n",
      "Episode  81 Current Replay Size 958\n",
      "Episode  82 Current Replay Size 1047\n",
      "Episode  83 Current Replay Size 1155\n",
      "Episode  84 Current Replay Size 1285\n",
      "Episode  85 Current Replay Size 1493\n",
      "Episode  86 Current Replay Size 1561\n",
      "Episode  87 Current Replay Size 1722\n",
      "Episode  88 Current Replay Size 1825\n",
      "Episode  89 Current Replay Size 1886\n",
      "Episode  90 Current Replay Size 1980\n",
      "Episode  91 Current Replay Size 2066\n",
      "Episode  92 Current Replay Size 2152\n",
      "Episode  93 Current Replay Size 2288\n",
      "Episode  94 Current Replay Size 2413\n",
      "Episode  95 Current Replay Size 2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 28.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.006751320095111926\n",
      "Episode  96 Current Replay Size 2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.004095372005055348\n",
      "Episode  97 Current Replay Size 2712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 28.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0038453453592956054\n",
      "Episode  98 Current Replay Size 2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0034623010247014445\n",
      "Episode  99 Current Replay Size 2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0031208518349255155\n",
      "Episode  100 Current Replay Size 3041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003584171658148987\n",
      "Reward  4.0\n",
      "Episode  101 Current Replay Size 3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0031397920843058568\n",
      "Episode  102 Current Replay Size 3274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0034436863362013025\n",
      "Episode  103 Current Replay Size 3394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003143645053884633\n",
      "Episode  104 Current Replay Size 3534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0031910801209354155\n",
      "Episode  105 Current Replay Size 3656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003964872657088564\n",
      "Episode  106 Current Replay Size 3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003737760594813156\n",
      "Episode  107 Current Replay Size 3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003236620596920449\n",
      "Episode  108 Current Replay Size 3993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0031662595784291638\n",
      "Episode  109 Current Replay Size 0\n",
      "Episode  110 Current Replay Size 100\n",
      "Episode  111 Current Replay Size 259\n",
      "Episode  112 Current Replay Size 329\n",
      "Episode  113 Current Replay Size 400\n",
      "Episode  114 Current Replay Size 527\n",
      "Episode  115 Current Replay Size 603\n",
      "Episode  116 Current Replay Size 711\n",
      "Episode  117 Current Replay Size 863\n",
      "Episode  118 Current Replay Size 974\n",
      "Episode  119 Current Replay Size 1040\n",
      "Episode  120 Current Replay Size 1131\n",
      "Episode  121 Current Replay Size 1239\n",
      "Episode  122 Current Replay Size 1355\n",
      "Episode  123 Current Replay Size 1485\n",
      "Episode  124 Current Replay Size 1577\n",
      "Episode  125 Current Replay Size 1690\n",
      "Episode  126 Current Replay Size 1787\n",
      "Episode  127 Current Replay Size 1873\n",
      "Episode  128 Current Replay Size 2006\n",
      "Episode  129 Current Replay Size 2150\n",
      "Episode  130 Current Replay Size 2260\n",
      "Episode  131 Current Replay Size 2358\n",
      "Episode  132 Current Replay Size 2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 28.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.007786204806373764\n",
      "Episode  133 Current Replay Size 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.004704908001391836\n",
      "Episode  134 Current Replay Size 2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.003810416118552286\n",
      "Episode  135 Current Replay Size 2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0033454056316986696\n",
      "Episode  136 Current Replay Size 2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0033483900909777715\n",
      "Episode  137 Current Replay Size 2935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 25.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0031082432573505984\n",
      "Episode  138 Current Replay Size 3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0032021496250914994\n",
      "Episode  139 Current Replay Size 3267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0027652905931851517\n",
      "Episode  140 Current Replay Size 3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 23.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 0.0027689226930184907\n",
      "Reward  4.0\n",
      "Episode  141 Current Replay Size 3483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/200 [00:01<00:06, 26.87it/s]"
     ]
    }
   ],
   "source": [
    "dqLearn(benv, exp_buf, dqn, 0.3, render_interval=10)\n",
    "#play(benv, dqn)\n",
    "benv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = step(benv, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benv.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'conv2d/kernel:0' shape=(8, 8, 2, 16) dtype=float32>, <tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32>, <tf.Variable 'conv2d_1/kernel:0' shape=(4, 4, 16, 32) dtype=float32>, <tf.Variable 'conv2d_1/bias:0' shape=(32,) dtype=float32>, <tf.Variable 'dense/kernel:0' shape=(2592, 256) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(256, 4) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array([1,1,1,1])\n",
    "actions = np.array([1,2,3,0])\n",
    "dqn = DQN((84, 84, 2), 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test3\n"
     ]
    }
   ],
   "source": [
    "dqn.save('test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from test\n"
     ]
    }
   ],
   "source": [
    "dqn.load('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([res[0][:,:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-943adc7f82b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "benv.step(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dqn.predict(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = res/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 0.        , 0.        , 0.10603726],\n",
       "        [0.        , 0.        , 0.        , 0.10603726]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "benv = gym.make('Breakout-v0')\n",
    "benv.get_action_meanings()\n",
    "\n",
    "state = np.array(benv.reset())\n",
    "#state = cv2.resize(cv2.cvtColor(state, cv2.COLOR_BGR2GRAY), dsize = (84, 110))[101 - 84:-9]\n",
    "print(state.shape)\n",
    "res = processImg([state, state])\n",
    "res = np.array([res, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 1.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "2 1.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "3 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "0 0.0\n",
      "2 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "3 0.0\n",
      "0 0.0\n",
      "0 0.0\n",
      "1 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "benv.render()\n",
    "done = False\n",
    "state = benv.reset()\n",
    "it = 0\n",
    "benv.step(1)\n",
    "while not done:\n",
    "    action = np.random.choice([0,1,2,3])\n",
    "    state, reward, done, p = benv.step(action)\n",
    "    it += 1\n",
    "    time.sleep(0.01)\n",
    "    benv.render()\n",
    "    print(action, reward)\n",
    "print(it)\n",
    "benv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_img, train_lbl), (test_img, test_lbl) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['TS', \"TR\", \"PL\", 'DR', \"CO\", \"SN\", 'SH', 'SN', 'BG', 'AB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_img[0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_img/255\n",
    "test_img = test_img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28, 28)),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_img, train_lbl, batch_size = 1000, epochs=5, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
